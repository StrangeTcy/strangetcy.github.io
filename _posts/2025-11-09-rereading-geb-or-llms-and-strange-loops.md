Re-reading GEB, or LLMs & Strange Loops

So, at the moment I'm re-reading GEB (for the fifth time, I think) to have something good to read come my birthday, 
and I inevitably find myself thinking about strange loops and modern LLMs. Also, this post (following my associations and wild idea generation)
might also touch upon finite state machines, geometry, "reasoning", prolog and other logical programming systems, and ethics.
At the moment I'm not sure how all this can be used to either jailbreak or secure the models, but that might come later.
Disclaimer: I've only experimented with deepseek-r1 via the web interface and GPT-5 (and its -mini versions) via the web interface.
Using the API or different models (like Claude) may produce different results. This is more of a musing and less a rigorous systematic research report.
Also, deepseek-r1 is great at reasoning, not that great at all at multimodal content, so I can't just feed it an Escher painting.


[And this is where the actual contents of my experiments should be described]
